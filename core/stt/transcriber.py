import logging
import os

import torch
from faster_whisper import WhisperModel

from core.config import SpeakNodeConfig

logger = logging.getLogger(__name__)

class Transcriber:
    def __init__(self, config: SpeakNodeConfig = None, model_size=None, device=None):
        """
        Whisper ëª¨ë¸ ì´ˆê¸°í™” (ì„œë²„ êµ¬ë™ ì‹œ 1íšŒ ì‹¤í–‰ë¨)
        configê°€ ì£¼ì–´ì§€ë©´ config ìš°ì„ , ì•„ë‹ˆë©´ ê°œë³„ ì¸ì ì‚¬ìš© (ì—­í˜¸í™˜)
        """
        cfg = config or SpeakNodeConfig()
        self.config = cfg
        self.language = cfg.whisper_language
        self.beam_size = cfg.whisper_beam_size
        _model_size = model_size or cfg.whisper_model

        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ (RunPod GPU ìš°ì„ )
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        # GPU ì‚¬ìš© ì‹œ float16, CPU ì‚¬ìš© ì‹œ int8 (ì†ë„ ìµœì í™”)
        compute_type = "float16" if self.device == "cuda" else "int8"
        
        logger.info("ğŸš€ [Transcriber] Loading model '%s' on %s (%s)...", _model_size, self.device, compute_type)
        
        try:
            # ëª¨ë¸ ë¡œë“œ (ë‹¤ìš´ë¡œë“œ ë° ìºì‹± ìë™ ì²˜ë¦¬)
            self.model = WhisperModel(
                _model_size, 
                device=self.device, 
                compute_type=compute_type
            )
            logger.info("âœ… [Transcriber] Model loaded ready.")
        except Exception as e:
            logger.critical("âŒ [Transcriber] Critical Error loading model: %s", e)
            raise

        # --- í™”ì ë¶„ë¦¬(Diarization) ì´ˆê¸°í™” (ì„ íƒì ) ---
        self.diarization_pipeline = None
        if cfg.enable_diarization and cfg.hf_token:
            try:
                from pyannote.audio import Pipeline as DiarizationPipeline
                logger.info("ğŸ™ï¸ [Transcriber] Loading Speaker Diarization model...")
                self.diarization_pipeline = DiarizationPipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    use_auth_token=cfg.hf_token,
                )
                if self.device == "cuda":
                    self.diarization_pipeline.to(torch.device("cuda"))
                logger.info("âœ… [Transcriber] Diarization model loaded.")
            except ImportError:
                logger.warning("âš ï¸ [Transcriber] pyannote.audio ë¯¸ì„¤ì¹˜. í™”ì ë¶„ë¦¬ ë¹„í™œì„±í™”.")
            except Exception as e:
                logger.warning("âš ï¸ [Transcriber] Diarization ë¡œë“œ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): %s", e)

    def _assign_speakers(self, segments: list[dict], diarization_result) -> list[dict]:
        """
        Diarization ê²°ê³¼ì™€ STT ì„¸ê·¸ë¨¼íŠ¸ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë§¤ì¹­í•˜ì—¬
        ê° ì„¸ê·¸ë¨¼íŠ¸ì— speaker í•„ë“œë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.
        """
        for seg in segments:
            seg_mid = (seg["start"] + seg["end"]) / 2.0
            best_speaker = "Unknown"
            best_overlap = 0.0

            for turn, _, speaker in diarization_result.itertracks(yield_label=True):
                # ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ê°„ì ì´ diarization turn ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
                overlap_start = max(seg["start"], turn.start)
                overlap_end = min(seg["end"], turn.end)
                overlap = max(0.0, overlap_end - overlap_start)

                if overlap > best_overlap:
                    best_overlap = overlap
                    best_speaker = speaker

            seg["speaker"] = best_speaker
        return segments

    def transcribe(self, audio_path: str) -> list[dict] | None:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ë°˜í™˜
        """
        if not os.path.exists(audio_path):
            logger.error("âš ï¸ [Error] File not found: %s", audio_path)
            return None

        logger.info("ğŸ§ [Transcriber] Processing audio: %s", os.path.basename(audio_path))
        
        # Transcribe ì‹¤í–‰
        segments, info = self.model.transcribe(
            audio_path, 
            beam_size=self.beam_size, 
            language=self.language,
            # 1. VAD í•„í„°ë¥¼ ë„ê±°ë‚˜, ì„ê³„ê°’ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.
            vad_filter=True, 
            vad_parameters=dict(
                min_silence_duration_ms=1000, # 1ì´ˆ ì´ìƒ ì¡°ìš©í•´ì•¼ ë¶„ë¦¬ (ê¸°ì¡´ 500msëŠ” ë„ˆë¬´ ì§§ìŒ)
                threshold=0.3                # ì†Œë¦¬ê°€ ì‘ì•„ë„ ìŒì„±ìœ¼ë¡œ ì¸ì‹í•˜ë„ë¡ ë¬¸í„±ê°’ ë‚®ì¶¤
            ),
            # 2. ë¬¸ì¥ ì¤‘ê°„ì— ëŠê¸°ëŠ” ê±¸ ë°©ì§€í•˜ê¸° ìœ„í•´ ì¶”ê°€
            condition_on_previous_text=True 
        )  
        logger.info("   â„¹ï¸ Detected language: '%s' (Probability: %.2f)", info.language, info.language_probability)
        
        # Generatorë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (DB ì €ì¥ìš© í¬ë§·íŒ…)
        result_data = []
        for segment in segments:
            # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì²˜ë¦¬
            if segment.text.strip():
                # ì½˜ì†”ì— ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                logger.debug("   [%.2fs -> %.2fs] %s", segment.start, segment.end, segment.text)
                
                result_data.append({
                    "start": segment.start,
                    "end": segment.end,
                    "text": segment.text.strip()
                })
        
        # --- í™”ì ë¶„ë¦¬ ì ìš© (í™œì„±í™”ëœ ê²½ìš°) ---
        if self.diarization_pipeline and result_data:
            try:
                logger.info("ğŸ™ï¸ [Transcriber] í™”ì ë¶„ë¦¬ ìˆ˜í–‰ ì¤‘...")
                diarization_result = self.diarization_pipeline(audio_path)
                result_data = self._assign_speakers(result_data, diarization_result)
                speaker_set = set(seg.get("speaker", "Unknown") for seg in result_data)
                logger.info("âœ… [Transcriber] í™”ì ë¶„ë¦¬ ì™„ë£Œ. ê°ì§€ëœ í™”ì: %s", speaker_set)
            except Exception as e:
                logger.warning("âš ï¸ [Transcriber] í™”ì ë¶„ë¦¬ ì‹¤íŒ¨ (STT ê²°ê³¼ëŠ” ìœ ì§€): %s", e)

        logger.info("âœ… [Transcriber] Completed. Total segments: %d", len(result_data))
        return result_data

# ==========================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì½”ë“œ (RunPodì—ì„œ ì§ì ‘ ì‹¤í–‰ ì‹œ ë™ì‘)
# ==========================================
if __name__ == "__main__":
    # 1. í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ ê²½ë¡œ (runpodctlë¡œ ì˜¬ë¦° íŒŒì¼ ì´ë¦„)
    TEST_FILE = "test_audio.mp3"  # ê°™ì€ í´ë”ì— ìˆë‹¤ê³  ê°€ì •
    
    if os.path.exists(TEST_FILE):
        # 2. ëª¨ë¸ ì´ˆê¸°í™” (ê°€ì¥ ê°•ë ¥í•œ large-v3 ëª¨ë¸ ì‚¬ìš©)
        # RunPod VRAMì´ ì¶©ë¶„í•˜ë¯€ë¡œ large-v3 ê¶Œì¥
        stt_engine = Transcriber(model_size="large-v3")
        
        # 3. ë³€í™˜ ìˆ˜í–‰
        results = stt_engine.transcribe(TEST_FILE)
        
        # 4. ê²°ê³¼ í™•ì¸
        print("\n--- [Final Result Sample] ---")
        print(results[:] if results else "No result") # ì•ë¶€ë¶„ 3ê°œë§Œ ì¶œë ¥
    else:
        print(f"âŒ '{TEST_FILE}' not found. Please upload it via runpodctl.")
        print("Tip: runpodctl send test_audio.mp3")